1 = Kafka Loader User Guide
2 
3 == Overview
4 
5 Kafka is a popular pub-sub system in enterprise IT, offering a distributed and fault-tolerant real-time data pipeline. TigerGraph's Kafka Loader feature lets you easily integrate with a Kafka cluster and speed up your real-time data ingestion. It is easily extensible using the many plugins available in the Kafka ecosystem.
6 
7 The Kafka Loader consumes data in a Kafka cluster and loads data into the TigerGraph system.
8 
9 == Architecture 
10 
11 From a high level, a user provides instructions to the TigerGraph system through GSQL, and the external Kafka cluster loads data into TigerGraph's RESTPP server. The following diagram demonstrates the Kafka Loader data architecture.
12 
13 image::kafka-loading-architecture.png[Kafka Data Loader Architecture]
14 
15 == Prerequisites
16 
17 You should have a Kafka cluster configured and set up in your environment.
18 
19 Once you have the external Kafka cluster setup, you need to prepare the following two configuration files and place them in your desired location in TigerGraph system:
20 
21 . Kafka data source configuration file: This file includes the external Kafka broker's domain name and port. Through the configuration file, TigerGraph system knows the location and port of the external Kafka broker.  Please see an example in link:#_1_define_the_data_source[*Step 1. Define the Data Source*].
22 . Kafka topic and partition configuration file: This file includes the Kafka topic, partition list, and start offset for the loading messages.  Please see an example in link:#_2_create_a_loading_job[*Step 2. Create a Loading Job*].
23 
24 == Configuring and Using the Kafka Loader
25 
26 There are three basic steps:
27 
28 . link:#_1_define_the_data_source[Define the data source]
29 . link:#_2_create_a_loading_job[Create a loading job]
30 . link:#_3_run_the_loading_job[Run the loading job]
31 
32 The GSQL syntax for the Kafka Loader is designed to be consistent with the existing GSQL loading syntax.
33 
34 === 1. Define the Data Source
35 
36 Before starting a Kafka data loading job, you need to define the Kafka server as a data source. The `CREATE DATA_SOURCE` statement defines a data_source variable with a subtype of KAFKA:
37 
38 [source,ruby]
39 ----
40 GSQL > CREATE DATA_SOURCE KAFKA data_source_name
41 ----
42 
43 *Specify Kafka Data Source Configuration File*
44 
45 After the data source is created, then use the `SET` command to specify the path to a configuration file for that data source.
46 
47 [source,ruby]
48 ----
49 GSQL > SET data_source_name = "/path/to/kafka.config"
50 ----
51 
52 This SET command reads, validates, and applies the configuration file, integrating its settings into TigerGraph's dictionary. The data source configuration file's content, structured as a JSON object, describes the Kafka server's global settings, including the data source IP and port. A sample kafka.conf is shown in the following example:
53 
54 [source,text]
55 ----
56 {
57   "broker": "192.168.1.11:9092",
58 }
59 ----
60 
61 The "broker" key is required, and its value is composite of a fully qualified domain name (or IP) and port. Additional Kafka configuration parameters may be provided (see Kafka documentation) by using the optional `"kafka_config"` key. For its value, provide a list of key-value pairs. For example:
62 
63 [source,text]
64 ----
65 {
66   "broker": "localhost:9092",
67   "kafka_config": {"group.id":"tigergraph"}
68 }
69 ----
70 
71 For simplicity, you can merge the `CREATE DATA_SOURCE` and `SET` statements:
72 
73 [source,ruby]
74 ----
75 GSQL > CREATE DATA_SOURCE KAFKA data_source_name = "/path/to/kafka.config"
76 ----
77 
78 [NOTE]
79 ====
80 
81 * If you have a TigerGraph cluster, the configuration file must be on machine m1, where the GSQL server and GSQL client both reside,  and it must be in JSON format. If the configuration file uses a relative path, the path should be relative to the GSQL client working directory.
82 * Each time when the config file is updated, you must run "SET data_source_name"  to update the data source details in the dictionary.
83 ====
84 
85 To further simplify, instead of specifying the Kafka data source config file path, you can also directly provide the Kafka data source configuration as a string argument, as shown below:
86 
87 [source,ruby]
88 ----
89 GSQL > CREATE DATA_SOURCE KAFKA data_source_name = "{\"broker\":\"broker.full.domain.name:9092\"}"
90 ----
91 
92 [NOTE]
93 ====
94 *Tip*: The above simplified statement is useful for using Kafka Data Loader in TigerGraph Cloud. In TigerGraph Cloud (tgcloud.io), you can use GSQL web shell to define and create Kafka data sources, without creating the Kafka data source configuration file in filesystem.
95 ====
96 
97 *Local data source vs global data source:*
98 
99 A data source can be either global or local:
100 
101 * A global data source can only be created by a superuser, who can grant it to any graph.
102 * An admin user can only create a local data source, which cannot be accessed by other graphs.
103 
104 The following are examples of permitted `DATA_SOURCE` operations.
105 
106 * Users with the `WRITE_DATASOURCE` privilege on the global scope may create a global level data source without assigning it to a particular graph:
107 
108 [source,ruby]
109 ----
110 GSQL > CREATE DATA_SOURCE KAFKA k1 = "/path/to/config"
111 ----
112 
113 * Users with the `WRITE_DATASOURCE` privilege on the global scope may grant/revoke a data source to/from one or more graphs:
114 
115 [source,ruby]
116 ----
117 GSQL > GRANT DATA_SOURCE k1 TO GRAPH graph1, graph2
118 GSQL > REVOKE DATA_SOURCE k1 FROM GRAPH graph1, graph2
119 ----
120 
121 * Users with the `WRITE_DATASOURCE` privilege for a particular graph user may create a local data source for that graph:
122 
123 [source,ruby]
124 ----
125 GSQL > CREATE DATA_SOURCE KAFKA k1 = "/path/to/config" FOR GRAPH test_graph
126 ----
127 
128 [NOTE]
129 ====
130 In the above statement, the local data_source k1 is only accessible to graph test_graph. A superuser cannot grant it to another graph**.**
131 ====
132 
133 ==== DROP DATA_SOURCE
134 
135 A data source variable can be dropped by a user who has privilege. A global data source can only be dropped by a users with global `WRITE_DATASOURCE` privilege. Users with `WRITE_DATASOURCE` privilege for one graph can drop data sources on that graph. The syntax for the `DROP DATA_SOURCE` command is as follows:
136 
137 [source,ruby]
138 ----
139 GSQL > DROP DATA_SOURCE <source1>[<source2>...] | * | ALL
140 ----
141 
142 Below are several examples of Kafka data source `CREATE` and `DROP` commands.
143 
144 [source,ruby]
145 ----
146 GSQL > CREATE DATA_SOURCE KAFKA k1 = "/home/tigergraph/kafka.conf"
147 GSQL > CREATE DATA_SOURCE KAFKA k2 = "/home/tigergraph/kafka2.conf"
148 
149 GSQL > DROP DATA_SOURCE k1, k2
150 GSQL > DROP DATA_SOURCE *
151 GSQL > DROP DATA_SOURCE ALL
152 ----
153 
154 ==== SHOW DATA_SOURCE
155 
156 The `SHOW DATA_SOURCE` command will display a summary of all existing data_sources for which the user has privilege:
157 
158 [source,c]
159 ----
160 GSQL > SHOW DATA_SOURCE *
161 
162 # the sample output
163 Data Source:
164   - KAFKA k1 ("127.0.0.1:9092")
165 The global data source will be shown in global scope. The graph scope will only show the data source it has access to.
166 ----
167 
168 === 2. Create a Loading Job
169 
170 The Kafka Loader uses the same basic https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/creating-a-loading-job#create-loading-job[CREATE LOADING JOB] syntax used for standard GSQL loading jobs. A `DEFINE FILENAME` statement should be used to assign a loader `FILENAME` variable to a Kafka data source name and the path to its config file.
171 
172 In addition, the filename can be specified in the `RUN LOADING JOB` statement with the `USING` clause. The filename value set by a `RUN` statement overrides the value set in the `CREATE LOADING JOB`.
173 
174 Below is the syntax for `DEFINE FILENAME` for use with the Kakfa Loader. In the syntax, `$DATA_SOURCE_NAME` is the Kafka data source name, and the path points to a configuration file with topic and partition information of the Kafka server. The Kafka configuration file must be in JSON format.
175 
176 [source,ruby]
177 ----
178 DEFINE FILENAME filevar "=" [filepath_string | data_source_string];
179 data_source_string = $DATA_SOURCE_NAME":"<path_to_configfile>
180 ----
181 
182 *Example:* _****_Load a Kafka Data Source _****_**k1**, _****_where the path to the topic-partition configuration file is `"~/topic_partition1_conf.json"`:
183 
184 [source,ruby]
185 ----
186 DEFINE FILENAME f1 = "$k1:~/topic_partition_config.json";
187 ----
188 
189 *Kafka Topic-Partition Configuration File*
190 
191 The topic-partition configuration file tells the TigerGraph system exactly which Kafka records to read.  Similar to the data source configuration file described above, the contents are in JSON object format. An example file is shown below:
192 
193 .topic_partition_config.json
194 
195 [source,yaml]
196 ----
197 {
198   "topic": "topicName1",
199   "partition_list": [
200     {
201       "start_offset": -1,
202       "partition": 0
203     },
204     {
205       "start_offset": -1,
206       "partition": 1
207     },
208     {
209       "start_offset": -1,
210       "partition": 2
211     }
212   ]
213 }
214 ----
215 
216 
217 
218 The `"topic"` key is required. Optionally,  a `"partition_list"` array can be included to specify which topic partitions to read and what start offsets to use.  If the `"partition_list"` key is missing or empty, all partitions in this topic will be used for loading. The default offset for loading is `"-1"`, which means you will load data from the most recent message in the topic, i.e., the end of the topic. If you want to load from the beginning of a topic, the "``start_offset"`` value should be "-2".
219 
220 You can also overwrite the default offset by setting `"default_start_offset"` in the Kafka topic configuration file. For example,
221 
222 [source,yaml]
223 ----
224 # all partition will be used if no "partition_list" item
225 {
226   "topic": "topicName1"
227 }
228 
229 # with empty "partition_list"
230 {
231   "topic": "topicName1",
232   "partition_list": []
233 }
234 
235 # overwrite the default start offset
236 {
237   "topic": "topicName1",
238   "default_start_offset": 0
239 }
240 ----
241 
242 Instead of specifying the config file path, you can also directly provide the topic-partition configuration as a string argument, as shown below:
243 
244 [source,text]
245 ----
246 DEFINE FILENAME f1 = "$k1:~/topic_partition_config.json";
247 DEFINE FILENAME f1 = "$k1:{\"topic\":\"zzz\",\"default_start_offset\":2,\"partition_list\":[]}";
248 ----
249 
250 === 3. Run the loading Job
251 
252 The Kafka Loader uses the same https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/running-a-loading-job#run-loading-job[RUN LOADING JOB] statement that is used for GSQL loading from files. Each filename variable can be assigned a string "DATA_SOURCE Var:topic_partition configure", which will override the value defined in the loading job. In the example below, the config files for f3 and f4 are being set by the RUN command, whereas f1 is using the config which was specified in the CREATE LOADING JOB statement.
253 
254 [source,ruby]
255 ----
256 RUN LOADING JOB job1 USING f1, f3="$k1:~/topic_part3_config.json", f4="$k1:~/topic_part4_config.json", EOF="true";
257 ----
258 
259 [CAUTION]
260 ====
261 A `RUN LOADING JOB` command may only use one type of data source.  E.g., you may not mix both Kafka data sources and regular file data sources in one loading job.
262 ====
263 
264 All filename variables in one loading job statement must refer to the same `DATA_SOURCE` variable.
265 
266 There are two modes for the Kafka Loader: streaming mode and EOF mode. The default mode is streaming mode.  In streaming mode, loading will never stop until the job is aborted. In EOF mode,  loading will stop after consuming the current Kafka message.
267 
268 To set EOF mode, an optional parameter is added to the `RUN LOADING JOB` syntax:
269 
270 [source,ruby]
271 ----
272 RUN LOADING JOB [-noprint] [-dryrun] [-n [i],j] jobname
273    [ USING filevar [="filepath_string"][, filevar [="filepath_string"]]*
274    [, CONCURRENCY="cnum"][,BATCH_SIZE="bnum"]][, EOF="true"]
275 ----
276 
277 To learn about each option and parameter of the `RUN LOADING JOB` command, see xref:3.2@gsql-ref:ddl-and-loading:running-a-loading-job.adoc#_options[Loading job options].
278 
279 == Manage Loading Jobs
280 
281 Kafka Loader loading jobs are managed the same way as regular loading jobs. The three key commands are
282 
283 * `SHOW LOADING STATUS`
284 * `ABORT LOADING JOB`
285 * `RESUME LOADING JOB`
286 
287 For example, the syntax for the `SHOW LOADING STATUS` command is as follows:
288 
289 [source,ruby]
290 ----
291 SHOW LOADING STATUS job_id|ALL
292 ----
293 
294 To refer to a specific job instance, using the job_id which is provided when `RUN LOADING JOB` is executed. For each loading job, the above command reports the following information :
295 
296 * Current loaded offset for each partition
297 * Average loading speed
298 * Loaded size
299 * Duration
300 
301 See https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/running-a-loading-job#inspecting-and-managing-loading-jobs[Inspecting and Managing Loading Jobs] for more details.
302 
303 == Kafka Loader Example
304 
305 Here is an example code for loading data through Kafka Loader:
306 
307 [source,ruby]
308 ----
309 USE GRAPH test_graph
310 DROP JOB load_person
311 DROP DATA_SOURCE k1
312 
313 #create data_source kafka k1 = "kafka_config.json" for graph test_graph
314 CREATE DATA_SOURCE KAFKA k1 FOR GRAPH test_graph
315 SET k1 = "kafka_config.json"
316 
317 # define the loading jobs
318 CREATE LOADING JOB load_person FOR GRAPH test_graph {
319   DEFINE FILENAME f1 = "$k1:topic_partition_config.json";
320   LOAD f1
321       TO VERTEX Person VALUES ($2, $0, $1),
322       TO EDGE Person2Comp VALUES ($0, $1, $2)
323       USING SEPARATOR=",";
324 }
325 
326 # load the data
327 RUN LOADING JOB load_person
328 ----
329 
330 ##
