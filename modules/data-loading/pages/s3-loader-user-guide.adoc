1 = AWS S3 Loader User Guide
2 
3 == Overview
4 
5 AWS Simple Storage Service (S3) is a popular destination to store data in the cloud and has become an essential component in the data pipeline of many enterprises. It is an object storage service on the AWS platform which can be accessed through a web service interface.
6 
7 TigerGraph's S3 Loader makes it easier for you to integrate with an Amazon S3 service and ingest data from S3 buckets either in _realtime_ or via _one-time data import_ into the TigerGraph System. Your TigerGraph cluster can be deployed either on-premises or in a cloud environment.
8 
9 == Architecture
10 
11 From a high level, a user provides instructions to the TigerGraph system through GSQL, and the external Amazon S3 data is loaded into TigerGraph's RESTPP server. The following diagram demonstrates the S3 Loader data architecture.
12 
13 image::graphguru15-image1.png[]
14 
15 == *Prerequisites*
16 
17 You should have uploaded your data to Amazon S3 buckets.
18 
19 Once you have the buckets set up, you need to prepare the following two configuration files and place them in your desired location in the TigerGraph system:
20 
21 . S3 data source configuration file: This file includes the credentials for accessing Amazon S3 which consists of _access key_ and _secret key_. Through the configuration file, the TigerGraph system acquires the authority to access your buckets. Please see the example in xref:s3-loader-user-guide.adoc#_1_define_the_data_source[Step 1. Define the Data Source].
22 . S3 file configuration file: This file specifies various options for reading data from Amazon S3. Please the see example in xref:data-loading:s3-loader-user-guide.adoc#_2_create_a_loading_job[Step 2. Create a Loading Job].
23 
24 == Configuring and Using the S3 Loader
25 
26 There are three basic steps:
27 
28 <<1. Define the Data Source>>
29 <<2. Create a Loading Job>>
30 <<3. Run the Loading Job>>
31 
32 The GSQL syntax for the S3 Loader is designed to be consistent with the existing GSQL loading syntax.
33 
34 === 1. Define the Data Source
35 
36 ==== CREATE DATA_SOURCE
37 
38 Before starting a S3 data loading job, you need to define the credentials to connect to Amazons  S3. The CREATE DATA_SOURCE statement defines a data_source type variable, with a sub type S3:
39 
40 [,ruby]
41 ----
42 CREATE DATA_SOURCE S3 data_source_name
43 ----
44 
45 ==== S3 Data Source Configuration File
46 
47 After the data source is created, use the SET command to specify the path to a configuration file for that data source.
48 
49 [,ruby]
50 ----
51 SET data_source_name = "/path/to/s3.config";
52 ----
53 
54 This SET command reads, validates, and applies the configuration file, integrating its settings into TigerGraph's dictionary. The data source configuration file's content, structured as a JSON object, describes the S3 credential settings, including the _access key_ and _secret key_. A sample s3.config is shown in the following example:
55 
56 .s3.config
57 
58 [,typescript]
59 ----
60 {
61     "file.reader.settings.fs.s3a.access.key": "AKIAJ****4YGHQ",
62     "file.reader.settings.fs.s3a.secret.key": "R8bli****p+dT4"
63 }
64 ----
65 
66 
67 
68 For simplicity, you can merge the CREATE DATA_SOURCE and SET statements:
69 
70 [,ruby]
71 ----
72 CREATE DATA_SOURCE S3 data_source_name = "/path/to/s3.config"
73 ----
74 
75 [NOTE]
76 ====
77 
78 . If you have a TigerGraph cluster, the configuration file must be on machine m1, where the GSQL server and GSQL client both reside,  and it must be in JSON format. If the configuration file uses a relative path, the path should be relative to the GSQL client working directory.
79 . Each time when the config file is updated, you must run "SET data_source_name"  to update the data source details in the dictionary.
80 ====
81 
82 To further simplify, instead of specifying the S3 data source config file path, you can also directly provide the S3 data source configuration as a string argument, as shown below:
83 
84 [,ruby]
85 ----
86 CREATE DATA_SOURCE S3 data_source_name = "{\"file.reader.settings.fs.s3a.access.key\":\"AKIAJ****4YGHQ\",\"file.reader.settings.fs.s3a.secret.key\":\"R8bli****p+dT4\"}"
87 ----
88 
89 [NOTE]
90 ====
91 *Tip*: The above simplified statement is useful for using S3 Data Loader in TigerGraph Cloud when you are not loading S3 data through GraphStudio. In TigerGraph Cloud (tgcloud.io), you can also use GSQL web shell to define and create S3 data sources, without creating the S3 data source configuration file in filesystem.
92 ====
93 
94 ==== ADVANCED: MultiGraph Support
95 
96 The S3 Loader supports the TigerGraph MultiGraph feature. In the MultiGraph context, a data source can be either global or local:
97 
98 . A global data source can only be created by a superuser, who can grant the global data source to any graph.
99 . An admin user can only create a local data source, which cannot be accessed by other graphs.
100 
101 The following are examples of permitted DATA_SOURCE operations.
102 
103 . A *superuser* may create a global level data source without assigning it to a particular graph:
104 
105 [,ruby]
106 ----
107 CREATE DATA_SOURCE S3 s1 = "/path/to/config"
108 ----
109 
110 ....
111 2. A **superuser** may grant/revoke a data source to/from one or more graphs:
112 ....
113 [,ruby]
114 ----
115 GRANT DATA_SOURCE s1 TO GRAPH graph1, graph2
116 REVOKE DATA_SOURCE s1 FROM GRAPH graph1, graph2
117 ----
118 
119 ....
120 3. An **admin** user may create a local data source for a specified graph which the admin user administers:
121 ....
122 [,ruby]
123 ----
124 CREATE DATA_SOURCE S3 s1 = "/path/to/config" FOR GRAPH test_graph
125 ----
126 
127 [NOTE]
128 ====
129 In the above statement, the local data_source s1 is only accessible to graph test_graph. A superuser cannot grant it to another graph**.**
130 ====
131 
132 ==== DROP DATA_SOURCE
133 
134 A data_source variable can be dropped by a user who has the privilege. A global data source can only be dropped by a superuser. A local data_source can only be dropped by an admin for the relevant graph or by a superuser. The syntax for the DROP command is as follows:
135 
136 [,ruby]
137 ----
138 DROP DATA_SOURCE <source1>[<source2>...] | * | ALL
139 ----
140 
141 Below is an example with a few legal s3 data_source create and drop commands.
142 
143 [,coffeescript]
144 ----
145 CREATE DATA_SOURCE S3 s1 = "/home/tigergraph/s3.config"
146 CREATE DATA_SOURCE S3 s2 = "/home/tigergraph/s3_2.config"
147 
148 DROP DATA_SOURCE s1, s2
149 DROP DATA_SOURCE *
150 DROP DATA_SOURCE ALL
151 ----
152 
153 ==== SHOW DATA_SOURCE
154 
155 The SHOW DATA_SOURCE command will display a summary of all existing data_sources for which the user has privilege:
156 
157 [,bash]
158 ----
159 $ GSQL SHOW DATA_SOURCE *
160 
161 # The sample output:
162 Data Source:
163   - S3 s1 ("file.reader.settings.fs.s3a.access.key": "AKIAJ****4YGHQ", "file.reader.settings.fs.s3a.secret.key": "R8bli****p+dT4")
164 # The global data source will be shown in global scope.
165 # The graph scope will only show the data source it has access to.
166 ----
167 
168 === 2. Create a Loading Job
169 
170 The S3 Loader uses the same basic https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/creating-a-loading-job#create-loading-job[CREATE LOADING JOB] syntax used for standard GSQL loading jobs. A DEFINE FILENAME statement should be used to assign a loader FILENAME variable to a S3 data source name and the path to its config file.
171 
172 In addition, the filename can be specified in the RUN LOADING JOB statement with the USING clause. The filename value set by a RUN statement overrides the value set in the CREATE LOADING JOB.
173 
174 Below is the syntax for DEFINE FILENAME when using the S3 Loader. In the syntax, $DATA_SOURCE_NAME is the S3 data source name, and the path points to a configuration file _which provides information about how to read an Amazon S3 file_. The S3 file configuration file must be in JSON format.
175 
176 [,ruby]
177 ----
178 DEFINE FILENAME filevar "=" [filepath_string | data_source_string];
179 data_source_string = $DATA_SOURCE_NAME":"<path_to_configfile>
180 ----
181 
182 _*Example:*_ Load a S3 Data Source _*s*_*1*, ___**___where the path to the file configuration file is "~/files.conf":
183 
184 [,ruby]
185 ----
186 DEFINE FILENAME f1 = "$s1:~/files.config";
187 ----
188 
189 ==== S3 File Configuration File
190 
191 The S3 file configuration file tells the TigerGraph system exactly which Amazon S3 files to read and how to read them. Similar to the data source configuration file described above, the contents are in JSON object format. An example file is shown below.
192 
193 .files.config
194 
195 [,typescript]
196 ----
197 {
198     "file.uris": "s3://my-bucket/data.csv"
199 }
200 ----
201 
202 
203 
204 The "file.uris" key is required. It specifies one or more paths on your Amazon S3 bucket. Each path is either to an individual file or to a directory. If it is a directory, then each file directly under that directory is included. You can specify multiple paths by using a comma-separated list. An example with multiple paths is show below:
205 
206 .files.config
207 
208 [,typescript]
209 ----
210 {
211     "file.uris": "s3://my-bucket1/data1.csv,s3://my-bucket1/data2.csv,s3://my-bucket2/data3.csv"
212 }
213 ----
214 
215 
216 
217 Instead of specifying the config file path, you can also directly provide the S3 file configuration as a string argument, as shown below:
218 
219 [,ruby]
220 ----
221 DEFINE FILENAME f1 = "$s1:~/files.config";
222 DEFINE FILENAME f1 = "$s1:{\"file.uris\":\"s3://my-bucket/data.csv\"}";
223 ----
224 
225 ==== ADVANCED: Configure How to Read S3 File
226 
227 Besides the required "file.uris" key, you can further configure the S3 loader. A sample full configuration is shown below:
228 
229 .files.config
230 
231 [,typescript]
232 ----
233 {
234     "tasks.max": 1,
235     "file.uris": "s3://my-bucket/data.csv",
236     "file.regexp": ".*",
237     "file.recursive": false,
238     "file.scan.interval.ms": 60000,
239     "file.reader.type": "text",
240     "file.reader.batch.size": 10000,
241     "file.reader.text.archive.type": "auto",
242     "file.reader.text.archive.extensions.tar": "tar",
243     "file.reader.text.archive.extensions.zip": "zip",
244     "file.reader.text.archive.extensions.gzip": "tar.gz,tgz"
245 }
246 ----
247 
248 
249 
250 Following is a detailed explanation of each option:
251 
252 * "*tasks.max*" (default is *1*): specifies the maximum number of tasks which can run in parallel. E.g. if there are 2 files and 2 tasks, each task will handle 1 file. If there are 2 files and 1 task, the single task will handle 2 files. If there is 1 file and 2 tasks, one of the tasks will handle the file.
253 * "*file.uris*": specifies the path(s) to the data files on Amazon S3. The path can also be dynamic by using expressions to modify the URIs at runtime. These expressions have the form `+${XX}+` where XX represents a pattern from https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html[`DateTimeFormatter`] Java class.
254 
255 [NOTE]
256 ====
257 if you want to ingest data dynamically, i.e. directories/files created every day and avoid adding new URIs every time, you can include expressions in URIs to do that. For example, for the URI``+s3://my-bucket/${yyyy}+``, it is converted to``s3://my-bucket/2019``when running the loader. You can use as many as you like in the URIs, for instance:``+s3://my-bucket/${yyyy}/${MM}/${DD}/${HH}-${mm}+``
258 ====
259 
260 * "*file.regexp*" (default is *.** which matches all files): the regular expression to filter which files to read.
261 * "*file.recursive*" (default is *false*): whether to recursively access all files in a directory.
262 * "*file.scan.interval.ms*" (default is *60000*): the wait time in ms before starting another scan of the file directory after finishing the current scan. Only applicable in *stream* mode.
263 * "*file.reader.type*" (default is *text*): the type of file reader to use. If *text*, read the file line by line as pure text. If *parquet*, read the file as parquet format.
264 * "*file.reader.batch.size*" (default is *1000*): maximum number of lines to include in a single batch.
265 * "*file.reader.text.archive.type*" (default is *auto*): the archive type of the file to be read. If *auto*, determine the archive type automatically. If *tar*, read the file with tar format. if *zip*, read the file with zip format. If *gzip*, read the file with gzip format. If *none*, read the file normally.
266 * "*file.reader.text.archive.extensions.tar*" (default is *tar*): the list of file extensions to be read with tar format.
267 * "*file.reader.text.archive.extensions.zip*" (default is *zip*):  __**__the list of file extensions to be read with zip format.
268 * "*file.reader.text.archive.extensions.gzip*" (default is *gzip*): the list of file extensions to be read with gzip format.
269 
270 [NOTE]
271 ====
272 The archive type is applied to all files in "file.uris" when loading. If you have different archive type files to be read at the same time, set *auto* for "file.reader.text.archive.type" and configure how to detect each archive extensions by providing the extensions list. Currently we support *tar*, *zip* and *gzip* archive types.
273 ====
274 
275 === 3. Run the Loading Job
276 
277 The S3 Loader uses the same https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/running-a-loading-job#run-loading-job[RUN LOADING JOB] statement that is used for GSQL loading from files. Each filename variable can be assigned a string "DATA_SOURCE Var:file configure", which will override the value defined in the loading job. In the example below, the config files for f2 and f3 are being set by the RUN command, whereas f1 is using the config which was specified in the CREATE LOADING JOB statement.
278 
279 [,ruby]
280 ----
281 RUN LOADING JOB job1 USING f1, f2="$s1:~/files1.config", f3="$s2:~/files2.config", EOF="true";
282 ----
283 
284 [CAUTION]
285 ====
286 A RUN LOADING JOB instance may only use one type of data source.  E.g., you may not mix both S3 data sources and regular file data sources in one loading job.
287 ====
288 
289 All filename variables in one loading job statement must refer to the same DATA_SOURCE variable.
290 
291 There are two modes for the S3 Loader: *streaming* mode and *EOF* mode. The default mode is *streaming* mode. In *streaming* mode, loading will never stop until the job is aborted. In *EOF* mode,  loading will stop after consuming the provided Amazon S3 file objects.
292 
293 To set *EOF* mode, an optional parameter is added to the RUN LOADING JOB syntax:
294 
295 [,ruby]
296 ----
297 RUN LOADING JOB [-noprint] [-dryrun] [-n [i],j] jobname
298    [ USING filevar [="filepath_string"][, filevar [="filepath_string"]]*
299    [, CONCURRENCY="cnum"][,BATCH_SIZE="bnum"]][, EOF="true"]
300 ----
301 
302 == Manage Loading Jobs
303 
304 S3 Loader loading jobs are managed the same way as native loader jobs. The three key commands are
305 
306 * SHOW LOADING STATUS
307 * ABORT LOADING JOB
308 * RESUME LOADING JOB
309 
310 For example, the syntax for the SHOW LOADING STATUS command is as follows:
311 
312 [,ruby]
313 ----
314 SHOW LOADING STATUS job_id|ALL
315 ----
316 
317 To refer to a specific job instance, use the job_id which is provided when RUN LOADING JOB is executed. For each loading job, the above command reports the following information :
318 
319 . current loaded lines
320 . average loading speed
321 . loaded size
322 . duration
323 
324 See https://docs.tigergraph.com/dev/gsql-ref/ddl-and-loading/running-a-loading-job#inspecting-and-managing-loading-jobs[Inspecting and Managing Loading Jobs] for more details.
325 
326 == S3 Loader Example
327 
328 Here is an example code for loading data through the S3 Loader:
329 
330 [,ruby]
331 ----
332 USE GRAPH test_graph
333 DROP JOB load_person
334 DROP DATA_SOURCE s1
335 
336 # Create data_source s3 s1 = "s3_config.json" for graph test_graph.
337 CREATE DATA_SOURCE S3 s1 FOR GRAPH test_graph
338 SET s1 = "s3_config.json"
339 
340 # Define the loading jobs.
341 CREATE LOADING JOB load_person FOR GRAPH test_graph {
342   DEFINE FILENAME f1 = "$s1:s3_file_config.json";
343   LOAD f1
344       TO VERTEX Person VALUES ($2, $0, $1),
345       TO EDGE Person2Comp VALUES ($0, $1, $2)
346       USING SEPARATOR=",";
347 }
348 
349 # load the data
350 RUN LOADING JOB load_person
351 ----
