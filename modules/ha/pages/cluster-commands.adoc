1 = Cluster Commands
2 
3 This page documents a list of advanced Linux commands that simplify platform operations that are performed often during debugging, especially on high availability (HA) clusters. Only the TigerGraph platform owner - the Linux user created during installation has access to the commands on this page.
4 
5 [NOTE]
6 ====
7 Users are advised to use these commands only at the guidance and recommendation of TigerGraph support. +
8 ====
9 
10 == Connection between nodes
11 
12 === Connect to another node via SSH
13 
14 [source,text]
15 ----
16 $ gssh <node_name>
17 ----
18 
19 This command allows you to connect to another node in your cluster via SSH.
20 
21 ==== Example:
22 
23 [source,bash]
24 ----
25 # Originally on m1
26 [tigergraph@ip-172-31-88-111 ~]$ gssh m3
27 Last login: Fri Apr 23 18:24:27 2021
28 # Now connected to m3 via ssh
29 [[tigergraph@ip-172-31-93-187 ~]$
30 ----
31 
32 
33 == File operations
34 
35 === Copy files on the specified nodes
36 
37 [source,bash]
38 ----
39 $ gscp <all|component_name|node_list> <source_path> <target_dir>
40 ----
41 
42 This command allows you to copy files from the current node to target folders on multiple nodes at the same time. The file or directory on the current node specified by the source path will be copied into the target folder on every node. If the target folder does not exist at the path given, the target folder will be created automatically. You can also specify multiple source files or directories, in which case, the source paths need to be absolute paths, put in quotes, and separated by space.
43 
44 You can specify the nodes where you want the copy operation to occur in the following ways:
45 
46 * `gscp all <source_path> <target_dir>` will execute the command on all nodes
47 * `gscp <component_name> <source_path> <target_dir>` will execute the command on nodes where the component you specified is running
48 * `gscp <node_list> <source_path> <target_dir>` will execute the command on the nodes you specify in the node list
49 
50 ==== Example
51 
52 [tabs]
53 ====
54 Single source::
55 +
56 --
57 
58 [source,cpp]
59 ----
60 $ gscp all /tmp/gscp_test /tmp/gscp_test_folder
61 
62 ### Connecting to local  server 172.31.91.54 ...
63 
64 ### Connecting to remote server 172.31.88.179 ...
65 
66 ### Connecting to remote server 172.31.91.208 ...
67 
68 // A copy of gscp_test is on every node
69 $ grun all 'ls /tmp/gscp_text_folder'
70 
71 ### Connecting to local  server 172.31.91.54 ...
72 gscp_test
73 
74 ### Connecting to remote server 172.31.88.179 ...
75 gscp_test
76 
77 ### Connecting to remote server 172.31.91.208 ...
78 gscp_test
79 
80 // Copy file to the target folder only on nodes where GPE is running
81 $ gscp gpe /tmp/gscp_test1 /tmp/gscp_test_folder
82 
83 // Copy file to a specified list of nodes
84 $ gscp m1,m2 /tmp/gscp_test3 /tmp/gscp_test_folder
85 ----
86 
87 --
88 
89 Multiple sources::
90 +
91 --
92 
93 [source,cpp]
94 ----
95 $ gscp all "/tmp/gscp_test1 /tmp/gscp_test2" /tmp/gscp_test_folder
96 
97 ### Connecting to local  server 172.31.91.54 ...
98 
99 ### Connecting to remote server 172.31.88.179 ...
100 
101 ### Connecting to remote server 172.31.91.208 ...
102 
103 // Copies of both files are on every node
104 $ grun all 'ls /tmp/gscp_text_folder'
105 
106 ### Connecting to local  server 172.31.91.54 ...
107 gscp_test1 gscp_test2
108 
109 ### Connecting to remote server 172.31.88.179 ...
110 gscp_test1 gscp_test2
111 
112 ### Connecting to remote server 172.31.91.208 ...
113 gscp_test1 gscp_test2
114 ----
115 
116 --
117 ====
118 
119 === Download file from another node
120 
121 [source,bash]
122 ----
123 $ gfetch <all|component_name|node_list> <source_path> <target_dir>
124 ----
125 
126 This command downloads a file or directory from every specified node to the target directory on the current node.
127 
128 ==== Example
129 
130 [source,cpp]
131 ----
132 $ gfetch all ~/test.txt ~/test_folder
133 
134 ### Connecting to local  server 172.31.91.54 ...
135 
136 ### Connecting to remote server 172.31.88.179 ...
137 
138 ### Connecting to remote server 172.31.91.208 ...
139 scp: /home/tigergraph/test.txt: No such file or directory
140 
141 // Nothing is downloaded if the file does not exist on a node
142 $ ls ~/test_folder
143 test.txt_m1  test.txt_m2
144 ----
145 
146 == Run commands on multiple nodes
147 
148 === Run commands sequentially
149 
150 [source,bash]
151 ----
152 $ grun <all|component_name|node_list> '<command>'
153 ----
154 
155 This command allows you to run commands on a specified list of nodes in your cluster one by one, and the output from every node will be visible to the terminal. `grun` will wait for the command to finish running on one node before executing the command on the next node.
156 
157 You can specify which nodes to run commands on in the following ways:
158 
159 * `grun all '<command>'` will execute the command on all nodes
160 * `grun <component_name> '<command>'` will execute the command on nodes where the component you specified is running
161 * `grun <node_list> '<command>'` will execute the commands on the nodes you specify in the node list
162 
163 ==== Example
164 
165 [tabs]
166 ====
167 All nodes::
168 +
169 --
170 
171 [source,bash]
172 ----
173 grun all 'echo "hello world"'
174 
175 ### Connecting to local  server 172.31.91.54 ...
176 hello world
177 
178 ### Connecting to remote server 172.31.88.179 ...
179 hello world
180 
181 ### Connecting to remote server 172.31.91.208 ...
182 hello world
183 ----
184 
185 --
186 
187 By component name::
188 +
189 --
190 
191 [source,bash]
192 ----
193 # Run 'echo "hello world"' on every node where GPE is running
194 grun gpe 'echo "hello world"'
195 
196 ### Connecting to local  server 172.31.91.54 ...
197 hello world
198 
199 ### Connecting to remote server 172.31.88.179 ...
200 hello world
201 
202 ### Connecting to remote server 172.31.91.208 ...
203 hello world
204 ----
205 
206 --
207 
208 By node list::
209 +
210 --
211 
212 [source,bash]
213 ----
214 grun m1,m3 'echo "hello world"'
215 
216 ### Connecting to local  server 172.31.91.54 ...
217 hello world
218 
219 ### Connecting to remote server 172.31.91.208 ...
220 hello world
221 ----
222 
223 --
224 ====
225 
226 === Run commands in parallel
227 
228 [source,bash]
229 ----
230 $ grun_p <all|component_name|node_list> '<command>'
231 ----
232 
233 This command allows you to run commands on a specified list of nodes in your cluster in parallel, and the output will be visible to the terminal where the `grun_p` command was run. You can specify the nodes to run commands on in the following ways:
234 
235 * `grun_p all '<command>'` will execute the command on all nodes
236 * `grun_p <component_name> '<command>'` will execute the command on nodes where the component you specified is running
237 * `grun_p <node_list> '<command>'` will execute the commands on the nodes you specify in the node list. The list of nodes should be separated by a comma, e.g.: `m1,m2`
238 
239 [tabs]
240 ====
241 All nodes ::
242 +
243 --
244 
245 [source,aspnet]
246 ----
247 $ grun_p all 'echo "hello world"'
248 
249 ### Connecting to local  server 172.31.91.54 ...
250 
251 ### Connecting to remote server 172.31.88.179 ...
252 
253 ### Connecting to remote server 172.31.91.208 ...
254 
255 ### ---- (m1)_172.31.91.54 ---0--
256 hello world
257 
258 ### ---- (m2)_172.31.88.179 ---0--
259 hello world
260 
261 ### ---- (m3)_172.31.91.208 ---0--
262 hello world
263 ----
264 
265 --
266 
267 By component::
268 +
269 --
270 
271 [source,console]
272 ----
273 $ grun_p gpe 'echo "hello world"'
274 
275 ### Connecting to local  server 172.31.91.54 ...
276 
277 ### Connecting to remote server 172.31.88.179 ...
278 
279 ### Connecting to remote server 172.31.91.208 ...
280 
281 ### ---- (m1)_172.31.91.54 ---0--
282 hello world
283 
284 ### ---- (m2)_172.31.88.179 ---0--
285 hello world
286 
287 ### ---- (m3)_172.31.91.208 ---0--
288 hello world
289 ----
290 
291 --
292 
293 By node list::
294 +
295 --
296 
297 [source,console]
298 ----
299 $ grun_p m1,m3 'echo "hello world"'
300 
301 ### Connecting to local  server 172.31.91.54 ...
302 
303 ### Connecting to remote server 172.31.91.208 ...
304 
305 ### ---- (m1)_172.31.91.54 ---0--
306 hello world
307 
308 ### ---- (m3)_172.31.91.208 ---0--
309 hello world
310 ----
311 
312 --
313 ====
314 
315 == Display cluster information
316 
317 === Show current node IP
318 
319 [source,bash]
320 ----
321 $ gmyip
322 ----
323 
324 This command returns the private IP address of your current node.
325 
326 ==== Example:
327 
328 [source,bash]
329 ----
330 $ gmyip
331 172.31.93.187 # Current node IP address
332 ----
333 
334 === Show current node number and servers
335 
336 [source,bash]
337 ----
338 $ ghostname
339 ----
340 
341 This command returns your current node number as well as all servers that are running on the current node.
342 
343 ==== Example
344 
345 In this example, `m1` is the current node number, and `ADMIN#1`, `admin#1` etc. are all servers that are running on `m1`.
346 
347 [source,cpp]
348 ----
349 $ ghostname
350 
351 m1 ADMIN#1 admin#1 CTRL#1 ctrl#1 DICT#1 dict#1 ETCD#1 etcd#1 EXE_1 exe_1 GPE_1#1 gpe_1#1 GSE_1#1 gse_1#1 GSQL#1 gsql#1 GUI#1 gui#1 IFM#1 ifm#1 KAFKA#1 kafka#1 KAFKACONN#1 kafkaconn#1 KAFKASTRM-LL_1 kafkastrm-ll_1 NGINX#1 nginx#1 RESTPP#1 restpp#1 TS3_1 ts3_1 TS3SERV#1 ts3serv#1 ZK#1 zk#1
352 ----
353 
354 === Show deployment information
355 
356 [source,bash]
357 ----
358 $ gssh
359 ----
360 
361 The `gssh` command, when used without arguments, outputs information about server deployments in your cluster. The output contains the names and IP addresses of every node. For each node, the output shows the full list of servers that are running on the node. For each server, the output shows the full list of the nodes that the server is running on.
362 
363 ==== Example
364 
365 [source,aspnet]
366 ----
367 $ gssh
368 
369 Usage: gssh m1|gpe_1#1|gse1_1#1|...
370 Usage: ----------------Available hosts--------------
371 Host *
372     IdentityFile /home/tigergraph/.ssh/tigergraph_rsa
373     Port 22
374 
375 Host m1 ADMIN#1 admin#1 CTRL#1 ctrl#1 DICT#1 dict#1 ETCD#1 etcd#1 EXE_1 exe_1 GPE_1#1 gpe_1#1 GSE_1#1 gse_1#1 GSQL#1 gsql#1 GUI#1 gui#1 IFM#1 ifm#1 KAFKA#1 kafka#1 KAFKACONN#1 kafkaconn#1 KAFKASTRM-LL_1 kafkastrm-ll_1 NGINX#1 nginx#1 RESTPP#1 restpp#1 TS3_1 ts3_1 TS3SERV#1 ts3serv#1 ZK#1 zk#1
376     HostName 172.31.91.54
377 
378 Host m2 ADMIN#2 admin#2 CTRL#2 ctrl#2 DICT#2 dict#2 ETCD#2 etcd#2 EXE_2 exe_2 GPE_2#1 gpe_2#1 GSE_2#1 gse_2#1 GSQL#2 gsql#2 GUI#2 gui#2 IFM#2 ifm#2 KAFKA#2 kafka#2 KAFKACONN#2 kafkaconn#2 KAFKASTRM-LL_2 kafkastrm-ll_2 NGINX#2 nginx#2 RESTPP#2 restpp#2 TS3_2 ts3_2 ZK#2 zk#2
379     HostName 172.31.88.179
380 
381 Host m3 ADMIN#3 admin#3 CTRL#3 ctrl#3 DICT#3 dict#3 ETCD#3 etcd#3 EXE_3 exe_3 GPE_3#1 gpe_3#1 GSE_3#1 gse_3#1 GSQL#3 gsql#3 GUI#3 gui#3 IFM#3 ifm#3 KAFKA#3 kafka#3 KAFKACONN#3 kafkaconn#3 KAFKASTRM-LL_3 kafkastrm-ll_3 NGINX#3 nginx#3 RESTPP#3 restpp#3 TS3_3 ts3_3 ZK#3 zk#3
382     HostName 172.31.91.208
383 
384 #cluster.nodes: m1:172.31.91.54,m2:172.31.88.179,m3:172.31.91.208
385 #admin.servers: m1,m2,m3
386 #ctrl.servers: m1,m2,m3
387 #dict.servers: m1,m2,m3
388 #etcd.servers: m1,m2,m3
389 #exe.servers: m1,m2,m3
390 #gpe.servers: m1,m2,m3
391 #gse.servers: m1,m2,m3
392 #gsql.servers: m1,m2,m3
393 #gui.servers: m1,m2,m3
394 #ifm.servers: m1,m2,m3
395 #kafka.servers: m1,m2,m3
396 #kafkaconn.servers: m1,m2,m3
397 #kafkastrm-ll.servers: m1,m2,m3
398 #nginx.servers: m1,m2,m3
399 #restpp.servers: m1,m2,m3
400 #ts3.servers: m1,m2,m3
401 #ts3serv.servers: m1
402 #zk.servers: m1,m2,m3
403 #log.root: /home/tigergraph/tigergraph/log
404 #app.root: /home/tigergraph/tigergraph/app/3.1.1
405 #data.root: /home/tigergraph/tigergraph/data
406 ----
407 
408 === Show graph status
409 
410 [source,text]
411 ----
412 $ gstatusgraph
413 ----
414 
415 This command returns the size of your data, the number of existing vertices and edges, as well as the number of deleted and skipped vertices on every node in your cluster. If you are running TigerGraph on a single node, it will return the same information that one node.
416 
417 ==== Single-node example
418 
419 [source,bash]
420 ----
421 $ gstatusgraph
422 === graph ===
423 [GRAPH  ] Graph was loaded (/home/tigergraph/tigergraph/data/gstore/0/part/):
424 [m1     ] Partition size: 437MiB, IDS size: 103MiB, Vertex count: 3181724, Edge count: 34512076, NumOfDeletedVertices: 0 NumOfSkippedVertices: 0
425 [WARN   ] Above vertex and edge counts are for internal use which show approximate topology size of the local graph partition. Use DML to get the correct graph topology information
426 ----
427 
428 ==== Cluster example
429 
430 [source,bash]
431 ----
432 $ gstatusgraph
433 === graph ===
434 [GRAPH  ] Graph was loaded (/home/tigergraph/tigergraph/data/gstore/0/part/):
435 [m1     ] Partition size: 246MiB, IDS size: 31MiB, Vertex count: 1152822, Edge count: 10908545, NumOfDeletedVertices: 0 NumOfSkippedVertices: 0
436 [m2     ] Partition size: 248MiB, IDS size: 31MiB, Vertex count: 1157325, Edge count: 11004342, NumOfDeletedVertices: 0 NumOfSkippedVertices: 0
437 [m3     ] Partition size: 225MiB, IDS size: 29MiB, Vertex count: 1049883, Edge count: 10009479, NumOfDeletedVertices: 0 NumOfSkippedVertices: 0
438 [WARN   ] Above vertex and edge counts are for internal use which show approximate topology size of the local graph partition. Use DML to get the correct graph topology information
439 ----
