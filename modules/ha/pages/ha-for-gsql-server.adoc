1 = High Availability Support for GSQL Server
2 
3 == *Introduction:*
4 
5 By Design, TigerGraph has built-in HA for all the internal critical components from the beginning. This includes GPE, GSE, REST API Servers, etc. However, the user-facing applications (GSQL and GraphStudio) were designed to be set up by customers based on their High Availability (HA) needs. This included building solutions using non-TigerGraph components.  With 3.1 release, TigerGraph will support native HA functionality for user-facing applications as well. This simplifies and streamlines HA deployment for users completely. For Operations personnel, this will reduce the operational overhead while enhancing the availability for end users.
6 
7 == *Overview of the design:*
8 
9 Before we elaborate the design, we need to understand the topology of how TigerGraph services are deployed in a cluster. TigerGraph nodes in a cluster are organized as '`m1`', '`m2`', and so on. Although all nodes in the cluster serve the same function - store data and participate in query execution, m1 is a special node. GSQL server runs on this node to address critical services such as storing client metadata as well as managing connections between client and server. With this feature, m1 will no longer serve as the only node for GSQL server connections. In the new design, other nodes will run standby GSQL servers to provide high availability for client connections.
10 
11 image::https://lh4.googleusercontent.com/pdt7MlbufmHMFuwTRX3LJev_aZjl0EgSu6XrfpNX8n2TpWSG8UFV2vS3mYSx-OO1opf_gFKCgFb3YDH6fQXMld-Zots3yApywDFBGmfFwlBVKR31bpAWdjrSmV8uegxe1WcchGFJ[]
12 
13 === *Role of Primary GSQL server*
14 
15 In the 3.1 release, primary GSQL server will continue to perform all the tasks handled by GSQL server prior to 3.1 release. This includes:
16 
17 . Process client connections
18 . Querying requests from GSQL clients
19 . User management requests including token management
20 
21 In addition to these, when Primary fails, a standby server will switch to become the Primary server, and when the old Primary server is back to normal function, it will become a GSQL Standby server.
22 
23 === *Role of Standby GSQL Servers*
24 
25 . Redirect requests to Primary Server
26 . Help Primary server to check for source data file existence and parse file header (if ANY is chosen)
27 
28 === *Role of GSQL Client*
29 
30 There is no change in how GSQL Client works.
31 
32 == *User Impact and Changes:*
33 
34 === *User Source Code Maintenance*
35 
36 Users store the following data on m1 node that is needed for query execution:
37 
38 * GSQL loader's Token functions
39 * ExprFunctions
40 * ExprUtil
41 
42 This is part of the user source code that TigerGraph system uses to compile. Prior to 3.1 release, this information is available to GSQL server only on m1 node. Typically, users can modify these files directly on the machine. But with HA, the Primary GSQL may not be in m1, and can be switched to any other machine at any time. Users have to make sure all the machines have the same content whenever there are updates to the files. This is a new requirement for users.
43 
44 GSQL server will retrieve the User source code files in the following priority order when it needs them:
45 
46 * Via github/github enterprise (if configuration is set),
47 * Files uploaded via PUT,
48 * Default files that are shipped with the product
49 
50 ==== *User source code in github code repository*
51 
52 This requires public network access, or github enterprise server access. User need to provide the following gadmin configuration:
53 
54 [source,text]
55 ----
56 GSQL.GithubUserAcessToken # the credential, or "anonymous", empty means not using github
57 GSQL.GithubRepository     # e.g. tigergraph/ecosys
58 GSQL.GithubBranch         # optional, o/w use "master" branch, e.g. demo_github
59 GSQL.GithubPath           # path to the directory in the github that has TokenBank.cpp, ExprFunctions.hpp, ExprUtil.hpp, e.g. sample_code/src
60 GSQL.GithubUrl            # optional, used for github enterprise, e.g. https://api.github.com
61 ----
62 
63 *Example:*
64 
65 [source,text]
66 ----
67 gadmin config set GSQL.GithubUserAcessToken anonymous
68 gadmin config set GSQL.GithubRepository tigergraph/ecosys
69 gadmin config set GSQL.GithubBranch demo_github
70 gadmin config set GSQL.GithubPath sample_code/src
71 gadmin config apply
72 ----
73 
74 When GSQL server needs to compile the files, it will retrieve them from github if the GitHub access is configured as above. It will retry 3 times, with timeout=5s for each time. If the connection fails, it will go to the next priority level method, i.e. file uploaded via PUT.
75 
76 ==== *User Source code maintenance for local files in the cluster:*
77 
78 We are introducing new GSQL commands to address this need. These commands will allow users to upload and download the user source files.
79 
80 *Upload source code*
81 
82 [source,text]
83 ----
84 PUT TokenBank FROM "path/to/a/file"
85 PUT ExprFunctions FROM "path/to/a/file"
86 PUT ExprUtil FROM "path/to/a/file"
87 ----
88 
89 *Example:*
90 
91 [source,text]
92 ----
93 temp_TokenBank=$tempDir/tmp_TokenBank.cpp
94 temp_ExprFunctions=$tempDir/tmp_ExprFunctions.hpp
95 temp_ExprUtil=$tempDir/tmp_ExprUtil.hpp
96 
97 eval gsql 'PUT TokenBank FROM \"$temp_TokenBank\"'
98 eval gsql 'PUT ExprFunctions FROM \"$temp_ExprFunctions\"'
99 eval gsql 'PUT ExprUtil FROM \"$temp_ExprUtil\"'
100 ----
101 
102 *Download source code*
103 
104 [source,text]
105 ----
106 GET TokenBank TO "path/to/a/file"
107 GET ExprFunctions TO "path/to/a/file"
108 GET ExprUtil TO "path/to/a/file"
109 ----
110 
111 *Example:*
112 
113 [source,text]
114 ----
115 temp_TokenBank2=$tempDir/tmp_TokenBank_2.cpp
116 temp_ExprFunctions2=$tempDir/tmp_ExprFunctions_2.hpp
117 temp_ExprUtil2=$tempDir/tmp_ExprUtil_2.hpp
118 
119 echo "GET TokenBank.cpp, ExprFunctions.hpp and ExprUtil.hpp to current node."
120 
121 eval gsql 'GET TokenBank TO \"$temp_TokenBank2\"'
122 eval gsql 'GET ExprFunctions TO \"$temp_ExprFunctions2\"'
123 eval gsql 'GET ExprUtil TO \"$temp_ExprUtil2\"'
124 ----
125 
126 The uploaded files will be saved to all nodes. Users will need to have either '`superuser`' and '`global_designer`' roles to have the sufficient privileges to run the PUT/GET commands.
127 
128 When calling GET command, the user can download the corresponding file from the Primary node, to a local directory at the current cluster node.
129 
130 When calling PUT command, the local file will be copied to all of the cluster nodes, including itself.
131 
132 *Example usage scenario to update of the files is as follows:*
133 
134 [source,text]
135 ----
136 // Download the current file via GET, or create a new file from draft;
137 GET TokenBank TO "/myFolder/file.cpp"
138 // Upload the file via PUT
139 PUT TokenBank FROM "/myFolder/file.cpp"
140 ----
141 
142 For each cluster node, TokenBank.cpp is stored at:
143 
144 [source,text]
145 ----
146  $(gadmin config get System.DataRoot)/gsql/tokenbank/
147 ----
148 
149 ExprFunctions.hpp and ExprUtil.hpp files are stored at:
150 
151 [source,text]
152 ----
153  $(gadmin config get System.DataRoot)/gsql/udf/
154 ----
155 
156 Full path should be provided including the file name for PUT/GET, eg:
157 
158 [source,text]
159 ----
160 put ExprFunctions from "/home/path/tmp/ExprFunc.hpp"
161 get TokenBank to "doc/path/tmp/myTB.cpp"
162 ----
163 
164 Notice that in the first command, we use absolute path, while in the second command, we use relative path. Both are supported. But "`~`" is not supported (eg: "`~/tmp/x.hpp`").
165 
166 Additionally, users can also use the commands in the following manner as well:
167 
168 * Use a folder name, and automatically default name will be added. For example:
169 
170 [source,text]
171 ----
172 put ExprFunctions from "/home/path/tmp"
173 ----
174 
175 It will use ExprFunctions.hpp under the directory "/home/path/tmp" for PUT.
176 
177 [source,text]
178 ----
179 get TokenBank to "home/path/tmp/"
180 ----
181 
182 It will create/overwrite the file "home/path/tmp/TokenBank.cpp".
183 
184 If the file name is given in the path, its file extension must be consistent with the corresponding file. For example:
185 
186 [source,text]
187 ----
188 put ExprFunctions from "/home/path/tmp/test1.gsql"
189 ----
190 
191 is not allowed, since PUT/GET ExprFunctions must use "`.hpp`" as file extension.
192 
193 ==== *Default file shipped with TigerGraph package*
194 
195 If the corresponding file is not found, the GSQL Primary server will use the default file in the package. These default files are at:
196 
197 [source,text]
198 ----
199 $(gadmin config get System.AppRoot)/dev/gdk/gsql/src/TokenBank/TokenBank.cpp
200 $(gadmin config get System.AppRoot)/dev/gdk/gsql/src/QueryUdf/ExprUtil.hpp
201 $(gadmin config get System.AppRoot)/dev/gdk/gsql/src/QueryUdf/ExprFunctions.hpp
202 ----
203 
204 === *File Path Configuration*
205 
206 In Pre-3.1 release design, the file path used in loading jobs refers to the file in m1, unless the user specifies machine name before the path (ALL, ANY, m1, m2,...). In the new HA design, the Primary server can be running on any machine, and can be switched. This means GSQL server may or may not find the file. To be back-compatible we prefix a machine name if the client is in TigerGraph cluster.
207 
208 Users can specify the node ID before the path using: ALL, ANY, m1, m2 and so forth. Declaring ALL or ANY as host ID will load files from every cluster node.
209 
210 User can use form like "`m1|m3|m4`" to declare the combination of several nodes.
211 
212 If the hosts are not specified, it will look for the host ID of the current node that is running the loading job, (through searching the nodes in $(gadmin config get GSQL.BasicConfig.Nodes)). If not found, it will use node "`m1`" by default.
213 
214 [source,text]
215 ----
216 # current refers to /path/to/csv in m1
217 LOAD "/path/to/csv" TO VERTEX vt VALUES($0)
218 LOAD "ALL:/path/to/csv" TO VERTEX vt VALUES($0)
219 LOAD "m1|m2:/path/to/csv" TO VERTEX vt VALUES($0)
220 ----
221 
222 Data source can be created and used with a file path or a JSON string, same as above.
223 
224 [source,text]
225 ----
226 create data_source kafka k1 for graph poc_graph
227 set k1 = "/tmp/kafka_config.json"
228 create data_source kafka k2 = "/tmp/kafka_config.json"
229 
230 CREATE LOADING JOB load_kafka FOR GRAPH poc_graph {
231   DEFINE FILENAME f1 = "$k1:/tmp/topic_partition_config.json";
232   LOAD f1
233       TO VERTEX MyNode VALUES ($2)
234       USING SEPARATOR="|";
235 }
236 ----
237 
238 === *GSQL Client connection setup:*
239 
240 GSQL client can connect to GSQL server in the different ways with the following priority order:
241 
242 ==== *Using IP address:*
243 
244 Users can specify the ip and port when calling GSQL client using "`gsql -i`" or "`gsql -ip`". For example:
245 
246 [source,text]
247 ----
248 gsql -ip 192.168.11.32:14240,192.168.11.34:14240,192.168.11.36
249 ----
250 
251 GSQL clients will try these ips and ports one by one. Notice the port is optional, it will use 14240 by default, which is the default port for GSQL server.
252 
253 ==== *Using GSQL IP Configuration:*
254 
255 If "`gsql -i`" or "`gsql -ip`" are not used, GSQL client will search the file gsql_server_ip_config where the user runs the GSQL client. The file gsql_server_ip_config should be a one-line file such as shown below. GSQL client will traverse the ips and ports in the file in its order.
256 
257 [source,text]
258 ----
259 172.18.0.101,172.18.0.102:14240,172.18.0.103:14240
260 ----
261 
262 Similarly, the port number is also optional, using 14240 by default.
263 
264 ==== *Using default local server:*
265 
266 If  "`gsql -i`" or "`gsql -ip`" are not used, and the file gsql_server_ip_config does not exist where "`gsql`" is called, GSQL client will try to connect to the local server (127.0.0.1:8123).
267 
268 === *Setting GSQL HA Configuration*
269 
270 Use gadmin config to get/set the following configurations related to GSQL High Availability.
271 
272 The first is the heartbeat interval in milliseconds. The second ("`max misses`") is the total timeout for switching to the Primary server which will measure the number of heartbeat intervals. It must be at least 2 to allow 1 heartbeat miss.
273 
274 [source,text]
275 ----
276 Controller.LeaderElectionHeartBeatIntervalMS = 2000
277 Controller.LeaderElectionHeartBeatMaxMiss = 4
278 ----
279 
280 For example, if we use "`IntervalMS = 2000`" and "`max misses = 4`" as shown above, then the total timeout is 2s×4 = 8 seconds. So the current Primary server will be switched if its heartbeat has stopped for more than 8 seconds. +
281 _**_
