1 = Cluster Shrinking
2 
3 Shrinking a cluster can make sense for a few reasons:
4 
5 * Your workload has changed and you can operate a cluster with fewer resources.
6 * Your data volume is lower than projected.
7 
8 Shrinking a cluster removes nodes from the cluster. The data stored on those nodes will be redistributed to the remaining nodes.
9 
10 == Before you begin
11 
12 * No loading jobs, queries, or REST requests are running on the new node or the cluster.
13 * Obtain a few key measures for the state of your data before shrinking, such as vertex counts/edge counts or certain query results. This will be useful in verifying data integrity after shrinking completes.
14 
15 == Procedure
16 
17 === Step 1: Identify new cluster replication and partition
18 
19 Before running any commands to shrink a cluster, make sure you have a clear idea of how the new cluster should be distributed. You should have the following information:
20 
21 * The new replication factor of the cluster
22 * The new partitioning factor of the cluster
23 * The names and IP addresses of the nodes to be removed from the cluster
24 
25 === Step 2: Shrink the cluster
26 
27 To shrink the cluster, run the `gadmin cluster shrink` command like below. If the shrinking involves changing the replication factor, use the `--ha <replicattion_factor>` option to indicate the new replication factor:
28 
29 [source,bash]
30 ----
31 $ gadmin cluster shrink node_ip_list [--ha <replication_factor>]
32 ----
33 
34 `node_ip_list` is the list of nodes you are removing from the cluster mapped to their IP addresses with a colon(`:`), and separated by a comma. Below is an example:
35 
36 [source,bash]
37 ----
38 $ gadmin cluster shrink m3:10.128.0.81,m4:10.128.0.82 --ha 1
39 ----
40 
41 ==== Supply a staging location
42 
43 Extra disk space is required during cluster shrinking. If more space is not available on the same disk, you can supply a staging location on a different disk to hold temporary data:
44 
45 [source,bash]
46 ----
47 $ gadmin cluster shrink m3:192.168.1.3,m4:192.168.1.4 --stagingPath /tmp/
48 ----
49 
50 If you choose to supply a staging location, make sure that the TigerGraph Linux user has write permission to the path you provide. The overall amount of space required for cluster shrinking on each node is `(1 + Math.ceil(oldPartition/newPartition) ) * dataRootSize`. `oldPartition` and `newPartition` each stand for the partitioning factor of the cluster before and after shrinking; `dataRootSize` stands for the size of the data root folder on the node.
51 
52 For example, if you are shrinking from a 10-node cluster with a replication factor of 2 and a partitioning factor of 5, to a 6-node cluster with a replication factor of 2 and a partitioning factor of 3, and the size of the data root folder on a node is 50GB. Then you would need more than `(1 + Math.ceil(5/3)) * 50) = 150 GB` of free space on the staging path.
53 
54 === Step 3: Verify success and delete temporary files
55 
56 When shrinking completes, you should see a message confirming the completion of the cluster change. The message will also include the location of the temporary files created during the operation.
57 
58 Verify data integrity by comparing vertex/edge counts or query results to what they were before the shrinking. After confirming a successful cluster contraction, delete the temporary files to free up disk space.
59 
60 === Step 4: Uninstall TigerGraph on the removed nodes
61 
62 Uninstall TigerGraph from the removed nodes by removing all root directories (`app`, `data` and `tmp`) except the log directory of TigerGraph.
63 
64 [CAUTION]
65 ====
66 For security reasons, `guninstall` is disabled on the removed nodes. Therefore, you need to delete TigerGraph by manually deleting the root directories.
67 ====
