1 = Cross-Region Replication
2 
3 TigerGraph's Cross-Region Replication (CRR) feature allows users to keep two or more TigerGraph clusters in different data centers or regions in sync. One cluster is the primary cluster, where users would perform all normal database operations, while the other is a read-only Disaster Recovery(CR) cluster that syncs with the primary cluster. CRR includes complete native support for syncing all data and metadata including automated schema, user, and query changes.
4 
5 image::https://lh5.googleusercontent.com/rfYwJYgfd2jHPJoBSgGy_ZoPc7DGnbE5VxHlBTaRFuAz2yFxPTzCF1kmha9VCuq2ZQQw5PZcFF6l07hJ-Oc1Nb2RN0j3ZtOjcGVwaCB07U63VAMCqWROG98iJE0KovUg5-_PTx2L[]
6 
7 For users of TigerGraph, cross-region replication will help deliver on the following business goals:
8 
9 * *Disaster Recovery*: Support Disaster Recovery functionality with the use of a dedicated remote cluster
10 * *Enhanced Availability*: Enhance Inter-cluster data availability by synchronizing data using Read Replicas across two clusters
11 * *Enhanced Performance*: If the customer application is spread over different regions, CRR can take advantage of data locality to avoid network latency.
12 * *Improved System Load-balancing*: CRR allows you to distribute computation load evenly across two clusters if the same data sets are accessed in both clusters.
13 * *Data Residency Compliance*: Cross-Region replication allows you to replicate data between different data centers or Regions to satisfy compliance requirements. Additionally, this feature can be used to set up clusters in the same region to satisfy more stringent Data sovereignty or localization business requirements.
14 * Besides providing disaster recovery and enhanced business continuity, CRR also allows you to set up the clusters as part of Blue/Green deployment purposes for agile upgrades.
15 
16 This page describes the procedure to set up a DR cluster, and the steps to perform a failover in the event of a disaster.
17 
18 == What is included
19 
20 The following information is automatically synced from the primary cluster to the DR cluster:
21 
22 * All data in every graph
23 * All graph schemas, including tag-based graphs
24 * All schema change jobs
25 * All users and roles
26 * All queries in every graph. Queries that are installed in the primary cluster will be automatically installed in the DR cluster.
27 
28 == Exclusions
29 
30 The following information and commands are *not* synced to the DR cluster
31 
32 * GraphStudio metadata
33  ** This includes graph layout data and user icons for GraphStudio.
34 * Loading jobs
35 * `gadmin` configurations
36 * `gsql --reset` command
37 * The following GSQL commands:
38  ** `EXPORT` and `IMPORT` commands
39  ** `DROP ALL` and `CLEAR GRAPH STORE`
40 
41 [WARNING]
42 ====
43 When the primary cluster executes an `IMPORT` , `DROP ALL`, or``CLEAR GRAPH STORE`` GSQL command, or the `gsql --reset` bash command, the services on the DR cluster will stop syncing with the primary and become outdated.
44 
45 See <<Sync an outdated DR cluster>> on how to bring an outdated DR cluster back in sync.
46 ====
47 
48 == Before you begin
49 
50 * Install TigerGraph 3.2 or higher on both the primary cluster and the DR cluster *in the same version*.
51 * Make sure that your DR cluster has the same number of partitions as the primary cluster.
52 
53 == *Setup*
54 
55 The following setup is needed in order to perform a failover in the event of a disaster.
56 
57 [NOTE]
58 ====
59 This setup assumes that you are setting up a DR cluster for an existing primary cluster. If you are setting up both the primary cluster and DR cluster from scratch, you only need perform Step 3 after TigerGraph is installed on both clusters.
60 ====
61 
62 === Step 1: Backup primary data
63 
64 Use GBAR to xref:backup-and-restore:backup-and-restore.adoc[create a backup] of the primary cluster. See xref:backup-and-restore:backup-and-restore.adoc[Backup and Restore] on how to create a backup.
65 
66 If you are setting up both the primary cluster and the DR cluster from scratch, you can skip Steps 1, 2, and 4 and only perform Step 3.
67 
68 === Step 2: Restore on the DR cluster
69 
70 Copy the backup files from every node to every node on the new cluster.  xref:backup-and-restore:backup-and-restore.adoc#restore[Restore the backup] of the primary cluster on the DR cluster. See xref:backup-and-restore:backup-and-restore.adoc[Backup and Restore] on how to restore a backup.
71 
72 === Step 3: Enable CRR on the DR cluster
73 
74 Run the following commands on the DR cluster to enable CRR on the DR cluster.
75 
76 [source,text]
77 ----
78 # Enable Kafka Mirrormaker
79 gadmin config set System.CrossRegionReplication.Enabled true
80 
81 # Kafka mirrormaker primary cluster's IPs, separator by ','
82 gadmin config set System.CrossRegionReplication.PrimaryKafkaIPs PRIMARY_IP1,PRIMARY_IP2,PRIMARY_IP3
83 
84 # Kafka mirrormaker primary cluster's KafkaPort
85 gadmin config set System.CrossRegionReplication.PrimaryKafkaPort 30002
86 
87 # The prefix of GPE/GUI/GSQL Kafka Topic, by default is empty.
88 gadmin config set System.CrossRegionReplication.TopicPrefix Primary
89 
90 # Apply the config changes, init Kafka, and restart
91 gadmin config apply -y
92 gadmin init kafka -y
93 gadmin restart all -y
94 ----
95 
96 === Step 4: Force install queries on primary
97 
98 Run the `INSTALL QUERY -force ALL` command on the primary cluster. After the command is finished, all other metadata operations on the primary cluster will start syncing to the DR cluster.
99 
100 == Restrictions on the DR cluster
101 
102 After being set up, the DR cluster will be read-only and all data update operations will be blocked. This includes the following operations:
103 
104 * All metadata operations
105  ** Schema changes
106  ** User access management operations
107  ** Query creation, installation, and dropping
108  ** User-defined function operations
109 * Data-loading operations
110  ** Loading jobs operations
111  ** RESTPP calls that modify graph data
112 * Queries that modify the graph
113 
114 == *Fail over to the DR cluster*
115 
116 In the event of catastrophic failure that has impacted the full cluster due to Data Center or Region failure, the user can initiate the failover to the DR cluster. This is a manual process. Users will have to make the following configuration changes on the DR cluster to upgrade it to the primary cluster.
117 
118 [source,text]
119 ----
120 gadmin config set System.CrossRegionReplication.Enabled false
121 gadmin config set System.CrossRegionReplication.PrimaryKafkaIPs
122 gadmin config set System.CrossRegionReplication.PrimaryKafkaPort
123 gadmin config set System.CrossRegionReplication.TopicPrefix Primary
124 gadmin config apply -y
125 gadmin restart -y
126 ----
127 
128 == Set up a new DR cluster after failover
129 
130 After you fail over to your DR cluster, your DR cluster is now the primary cluster. You may want to set up a new DR cluster to still be able to recover your services in the event of another disaster.
131 
132 To set up a new DR cluster over the upgraded primary cluster:
133 
134 . Make a backup of the upgraded primary cluster
135 . Run the following command on the new cluster. The commands are the mostly same as setting up the first DR cluster, except that in the fourth command, the value for `System.CrossRegionReplication.TopicPrefix` becomes `Primary.Primary` instead of `Primary`
136 . On the new DR cluster, restore from the backup of the upgraded primary cluster
137 
138 [source,text]
139 ----
140 # Enable Kafka Mirrormaker
141 gadmin config set System.CrossRegionReplication.Enabled true
142 
143 # Kafka mirrormaker primary cluster's IPs, separator by ','
144 gadmin config set System.CrossRegionReplication.PrimaryKafkaIPs PRIMARY_IP1,PRIMARY_IP2,PRIMARY_IP3
145 
146 # Kafka mirrormaker primary cluster's KafkaPort
147 gadmin config set System.CrossRegionReplication.PrimaryKafkaPort 30002
148 
149 # The prefix of GPE/GUI/GSQL Kafka Topic, by default is empty.
150 gadmin config set System.CrossRegionReplication.TopicPrefix Primary.Primary
151 
152 # Apply the config changes, init Kafka, and restart
153 gadmin config apply -y
154 gadmin init kafka -y
155 gadmin restart all -y
156 ----
157 
158 There is no limit on the number of times a cluster can fail over to another cluster. When designating a new DR cluster, make sure that you set the `System.CrossRegionReplication.TopicPrefix` parameter correctly by adding an additional `.Primary` .
159 
160 For example, if your original cluster fails over once, and the current cluster's `TopicPrefix` is `Primary`, then the new DR cluster needs to have its `TopicPrefix` be `Primary.Primary`. If it needs to fail over again, the new DR cluster needs to have its `TopicPrefix` be set to `Primary.Primary.Primary`.
161 
162 == Sync an outdated DR cluster
163 
164 When the primary cluster executes an `IMPORT`, `DROP ALL`, or `CLEAR GRAPH STORE` GSQL command, or the `gsql --reset` bash command, the services on the DR cluster will stop syncing with the primary and become outdated.
165 
166 To bring an outdated cluster back in sync, you need to generate a fresh backup of the primary cluster, and perform the link:#_setup[setup steps] again. However, you can skip Step 3: Enable CRR on the DR cluster, because CRR will have already been enabled.
